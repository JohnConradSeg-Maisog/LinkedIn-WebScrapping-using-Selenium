Title,CompanyDetails,JobDetails,JobDescription,JobSkills
Data Engineer-Business Intelligence,"IBM · Cebu, Central Visayas, Philippines 1 week ago · 5 applicants",On-site Full-time,"About the job
676066BR

Introduction

In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.

A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.

You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.

Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.

Your Role and Responsibilities

Design, build and manage solutions using Web-based BI reporting technology that offers personalized features, such as allowing users to drill down into the content of a report using a menu and comes with an intuitive design kit. Includes integration between Business Intelligence Platforms, which will allow users to toggle easily between reporting and analysis tasks. The reports can be distributed over the Web, via e-mail or through a portal or file server. Skills include: Business intelligence, Cognos, Reporting, Actuate, MicroStrategy, Dashboards, Tableau, Click, Scorecards. PH_DIGhotjobs

Required Technical and Professional Expertise

Must Have Skills


Experience in Tableau
Data Visualization and manipulation
Creating Tableau Dashboards
Maintaining and monitoring Tableau dashboards server
Experience in Power BI and Azure

Preferred Technical And Professional Expertise

Nice to have:


Azure


About Business Unit

IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

Your Life @ IBM

In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, caste, genetics, pregnancy, disability, neurodivergence, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.","Analytics
Add,Dashboards
Add,Data Analytics
Add,Data Engineering
Add,Data Science
Add,Data Visualization
Add,Extract, Transform, Load (ETL)
Add,MicroStrategy
Add,Microsoft Azure
Add,Tableau
Add,"
Business Intelligence Developer / Data Engineer,"Teradyne · Lapu-Lapu, Central Visayas, Philippines Reposted 2 weeks ago · 18 applicants",On-site Full-time Mid-Senior level,"About the job
We are the global test and automation specialists, powering next-generation technologies through sophisticated solutions. Behind every electronic device you use, Teradyne's test technology ensures your device works right the first time, every time! Our portfolio of automation solutions help manufacturers to develop and deliver products quickly, efficiently and cost-effectively. Teradyne companies deliver manufacturing automation across industries and applications around the world!

 The Business Intelligence Developer / Data Engineer will have the following tasks: 

 Evaluates requirements and designs effective data solutions to meet business needs.
 Develops and facilitates implementation of data solutions within the development team and throughout the organization.
 Coordinate with the rest of the IT group and business users in selecting the appropriate solution(s) for each requirement.
 Works with business stakeholders to ensure tight fit between tools and business requirements. Manage and work with other developers to build standard development processes and reusable components.
 Develops the best practices in the development of data solutions
 Advocate for the continuous improvement of the standards and best practices within the area of Data Solutions and Integration
 Troubleshoots data tool problems and tunes for performance.
 Assists development and management of training, documentation, and help desk capabilities.

 The BI Developer/Data Engineer must meet the following mandatory requirements: 

 3 to 5 years of relevant experience required. Power BI or Tableau experience preferred.
 In depth knowledge of data warehousing theories and techniques and multi-dimensional design
 Strong knowledge in ETL (Extract Transform Load) methodologies using tools/scripts and/or have experience doing data integration using ETL tools
 Ability to comprehend complex technical and business concepts and adapt quickly to changes.
 Proven visual design, systematic and problem solving skills
 Strong written and verbal English communication skills
 Strong knowledge of SQL
 Basic project management skills

Nice To Have

 Knowledge in Informatica, SSIS and/or Snowflake
 Degree or working knowledge in Statistics and Higher math
 Knowledge in Big Data / Data Science
 Experience on Azure cloud
 In-depth knowledge in Power BI, SSIS, SSAS and Visual Studio.

 We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment.","Analytics
Add,Business Intelligence (BI)
Add,Communication
Add,DAX
Add,Data Analytics
Add,Data Warehousing
Add,English
Add,Problem Solving
Add,SQL Server Analysis Services (SSAS)
Add,SQL Server Integration Services (SSIS)
Add,"
Senior Data Engineer,"Manulife · Makati, National Capital Region, Philippines Reposted 3 days ago · 38 applicants",Full-time Mid-Senior level,"About the job
Are you looking for a supportive, collaborative workplace with great teams and inspiring leaders? You’ve come to the right place. We’re looking for ambitious people who share our values and want to make every day better for people around the world. If this sounds like you, and the career below sounds exciting, we’d like to hear from you.

Working Arrangement

Hybrid

Job Description

6+ years related working experience
Proficient understanding of distributed computing principles
Good Knowledge of Hadoop cluster, Azure, with all included services
Ability to solve any ongoing issues with operating the cluster
Proficiency with Hadoop v2, MapReduce, HDFS
Experience with Azure ADLSg2, Data Factory, DataBricks, Eventhub,
Experience with data engineering projects, i.e integration of data from multiple data sources to a target system
Good knowledge of Big Data querying tools – Hive, HBase, SQL
Knowledge in data streaming (e.g. Kafka, Spark)
Knowledge in any server-side programming languages (e.g. Java, Python, Spark, R…etc)
Knowledge in any Change Data Capture (CDC) solutions
Experience in working on Agile projects is an advantage.
Experience on Azure Cloud and/or other Cloud platforms
Open Source RDBMS management including MongoDB & HBase
Experience in managing Zeppelin, JupyterHub, R Studio, Neo4j are definitely added advantage
Selecting and integrating any Big Data tools and frameworks required providing requested capabilities
Implement ETL/ELT flows via Big Data Solutions i.e Nifi, Spark, Python, Azure Data Factory, Azure Synapse, Azure, DataBricks…etc
To be able to monitor the performance and advice necessary infrastructure changes or performance tuning in codes.
Support business users in use of the Enterprise Data Lake (EDL) as part of BAU
Perform POC on new integration patterns and solutions.
Maintain and monitor platform stability and performance of EDL
Attends to Incidents and change requests.
Write and maintain technical documentation.
Perform unit tests and system integration tests.
Executes updates, patches, and other activities required to maintain and enhance the operations of the EDL..
Supports the Agile delivery squads when required.

Every career at Manulife/John Hancock provides the opportunity to learn new skills and move your career forward. Ready to make an impact somewhere? What are you waiting for? Apply today.

About John Hancock And Manulife

John Hancock is a unit of Manulife Financial Corporation, a leading international financial services group that helps people make their decisions easier and lives better. We operate primarily as John Hancock in the United States, and Manulife globally, including Canada, Asia and Europe. We provide financial advice, insurance and wealth and asset management solutions for individuals, groups and institutions. Assets under management and administration by Manulife and its subsidiaries were CAD$1.3 trillion (US$1.1 trillion) as of June 30, 2021. Manulife Financial Corporation trades as MFC on the TSX, NYSE, and PSE, and under 945 on the SEHK. Manulife can be found at manulife.com.

One of the largest life insurers in the United States, John Hancock supports more than 10 million Americans with a broad range of financial products, including life insurance, annuities, investments, 401(k) plans, and education savings plans. Additional information about John Hancock may be found at johnhancock.com.

Manulife is an Equal Opportunity Employer

At Manulife/John Hancock, we embrace our diversity. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention, advancement and compensation, and we administer all of our practices and programs without discrimination on the basis of race, ancestry, place of origin, colour, ethnic origin, citizenship, religion or religious beliefs, creed, sex (including pregnancy and pregnancy-related conditions), sexual orientation, genetic characteristics, veteran status, gender identity, gender expression, age, marital status, family status, disability, or any other ground protected by applicable law.

It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will work with applicants who request a reasonable accommodation during the application process. All information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and Manulife/John Hancock policies. To request a reasonable accommodation in the application process, contact recruitment@manulife.com.","Analytics
Add,Azure Data Factory
Add,Big Data
Add,Change Data Capture
Add,Data Engineering
Add,Data Integration
Add,Hive
Add,Neo4j
Add,RStudio
Add,Technical Documentation
Add,"
Business Intelligence Data Engineer II,Concentrix · Metro Manila Reposted 1 week ago · 37 applicants,On-site Full-time Entry level,"About the job
Job Title:

Business Intelligence Data Engineer II

Job Description

Location:

PHL Work-at-Home NCR Muntinlupa

Language Requirements:

Time Type:

Full time

If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents

R1411348","Data Engineering
Add,Data Modeling
Add,Data Science
Add,Data Warehousing
Add,Databases
Add,Extract, Transform, Load (ETL)
Add,"
Data Engineer (WFH),"Michael Page · Manila, National Capital Region, Philippines 6 months ago · 237 applicants",Remote Full-time Entry level,"About the job
Tech Startup|Emerging Technology


About Our Client

Looking for a pioneering team here in the country.

Job Description

Developing high performance solutions with best practices on tools used
Completing full test cycle before implementation
Develop data pipeplines
Communicating with the stakeholders to analyze and understand the need and design the solution accordingly


The Successful Applicant

A successful Data Engineer has a strong working knowledge of business trends and is able to read and analyze product, market, and share trends. You should have strong organizational, critical thinking, and communication skills.

Experience Azure Data Factory
Thorough understanding of data warehousing, data modelling techniques and entity relationship diagram
Data Analysis and cleansing techniques
Advanced skills of SQL
Experience on data transformation techniques


What's on Offer

If you are looking a challenging and fulfilling role in data and analytics, please do not hesitate to reach out!

Contact: Raian Mondelo

Quote job ref: JN-062023-6074604","Data Analysis,Communication
Add,Critical Thinking
Add,Data Analytics
Add,Data Engineering
Add,Data Modeling
Add,Data Warehousing
Add,ERD
Add,Modeling
Add,SQL
Add,"
Senior Data Engineer,"eTeam · National Capital Region, Philippines 1 week ago · 32 applicants",Remote Full-time Mid-Senior level,"About the job
Designs and builds solutions to move data from operational and external environments to the business intelligence environment using Informatica, Ab Initio software and DataStage 
(formerly Ascential) - IBM's WebSphere Data Integration Suite. Skills include designing and developing extract, transform and load (ETL) processes. Experience includes full lifecycle 
implementation of the technical components of a business intelligence solution.","Analytics
Add,Big Data
Add,Cascading Style Sheets (CSS)
Add,Data Analytics
Add,Data Engineering
Add,Data Science
Add,Data Warehousing
Add,DataStage
Add,Databases
Add,Extract, Transform, Load (ETL)
Add,"
AI/ML Developer,"CGI · Taguig, National Capital Region, Philippines Reposted 2 weeks ago · 263 applicants",Full-time Entry level,"About the job
Position Description

We are looking for an experienced Developer to be responsible for performing tasks based on project requirements and supporting various development duties. The responsibilities of Developers include writing code, analyzing data, and contributing to the design and implementation of software.

Your future duties and responsibilities

To be successful as a Developer, you should demonstrate great skill in creativity and innovation, ability to thrive in a high-pressure environment, and possess excellent communication skills. Ultimately, a top-notch Developer should have extensive experience in software development, be able to keep up to date with deadlines, and have strong analytical skills.

Your future duties and responsibilities

Performing coding assignments.

Reviewing code work for accuracy and functionality.

Creating and implementing design plans.

Analyzing code segments regularly.

Keeping up-to-date with industry trends and technology developments.

Required Qualifications To Be Successful In This Role

Degree in Information Technology, Computer Science, or related.

Extensive knowledge of software development and its technologies.

Data Engineering / ETL

Data Analysis

Data Visualization

Strong knowledge of Python modules like Tensorflow, Keras, PyTorch, scikit-learn or other similar modules

Solid experience in coding.

Good communication skills.

Good time management skills.

Insights you can act on

While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.

When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees “members” because they are CGI shareholders and owners, and, as owners, we enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today—one of the world’s largest independent providers of IT and business consulting services.

At CGI, we recognize the richness that diversity brings. We strive to create a work culture where everyone belongs, and we collaborate with clients in building more inclusive communities. As an equal opportunity employer, we empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.

Ready to become part of our success story? Join CGI—where your ideas and actions make a difference.","Data Analysis,Communication
Add,Computer Science
Add,Creativity and Innovation
Add,Data Analytics
Add,Data Science
Add,High Pressure
Add,Scikit-Learn
Add,Software Development
Add,Software Implementation
Add,"
GDS Consulting_Senior Data Engineer 1,"EY · Taguig, National Capital Region, Philippines Reposted 1 week ago · 25 applicants",On-site Full-time Mid-Senior level,"About the job
At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.

 5 to 10 years of Experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
 Extensive hands-on experience implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Databricks Azure Data Catalog, Cosmo Db etc.
 Hands on experience in programming like python, scala
 Must have hands on experience in SQL and procedural SQL languages
 Experience working with NoSQL in at least one of the data stores - HBase, Cassandra, MongoDB
 Strong analytical skills and enjoys solving complex technical problems
 Proficiency in Software Development Best Practices
 Strong analytical skills and enjoys solving complex technical problems
 Excellent debugging and optimization skills
 Experience in Enterprise grade solution implementations & in converting business problems/challenges to technical solutions considering security, performance, scalability etc
 Designing and implementing data ingestion pipelines from multiple sources using Azure or AWS Databricks.
 Developing scalable and re-usable frameworks for ingesting of data sets.
 Integrating the end to end data pipeline - to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times.
 Participating in development of cloud data warehouses, data as a service, business intelligence solutions.
 Data wrangling of heterogeneous data.

EY | Building a better working world

EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.

Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.

Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.","Big Data
Add,Cassandra
Add,Data Ingestion
Add,Data Warehousing
Add,HBase
Add,MongoDB
Add,NoSQL
Add,Relational Databases
Add,SQL
Add,Scala
Add,"
Analytics Engineer II,Remitly · Metro Manila Reposted 6 days ago · 22 applicants,On-site Full-time Entry level,"About the job
Job Description

Remitly is on a mission to transform the lives of immigrants and their families by providing the most trusted financial products and services on the planet. Since 2011, we have been tirelessly delivering on our promises to immigrants sending their hard earned money home. Today, we are reimagining international payments at scale and building new products to create deeper relationships with our customers and their loved ones across the globe. Join over 2,700 employees across 10 offices who are growing their careers while having a positive impact on people globally.

About The Role

The Reconciliation Insights and Analytics team builds important financial products and tools that ensure the accuracy and completeness of our financial data. We focus on analyzing and detecting data discrepancies, improving customer experience, and delivering business insight. Our data supports data consumers like Accounting, Treasury, Finance, and Customer Success. You will report to BI Manager, Insights & Analytics.

You Will
Work with teams to understand requirements and ensure that data solutions align with Remitly's goals
Develop data dictionaries, data models, and other documentation to support the Reconciliation cash flow dataset data infrastructure
Ensure the consistency and accuracy of Reconciliation cash flow dataset across all reporting systems, resolving data quality issues
Collaborate with data engineering teams to develop data pipelines and ETL processes that ensure efficient data ingestion and processing
Monitor and analyze data to identify trends, patterns, and anomalies, providing recommendations to inform decisions
Communicate complex data insights and technical information to non-technical partners, both verbally and through data visualizations
Design end-to-end BI solutions that allow partners to analyze the cash flow dataset, identify new opportunities, and improve cash flow operations.
Identifies ways to improve data reliability, efficiency, and quality of data management required for building BI applications and reports.
You Have
5+ years as an analytics engineer or equivalent
Knowledge of logging, error handling, and familiarity with CI/CD concepts.
Strong communication skills and ability to collaborate effectively with stakeholders.
Experience in dimensional data modeling, ETL design, implementation and maintenance
Expert in SQL, git, and a programming language (ie Python, Java, etc)
Experience with Airflow, Snowflake/Redshift, Spark
Experience with analytical model development and deployment processes
Familiarity with AWS products and services
You live our cultural values
We are committed to nondiscrimination across our global organization and in all of our business operations. Employment is determined based upon personal capabilities and qualifications without discrimination on the basis of race, creed, color, religion, sex, gender identification and expression, marital status, military status or status as an honorably discharge/veteran, pregnancy (including a woman's potential to get pregnant, pregnancy-related conditions, and childbearing), sexual orientation, age (40 and over), national origin, ancestry, citizenship or immigration status, physical, mental, or sensory disability (including the use of a trained dog guide or service animal), HIV/AIDS or hepatitis C status, genetic information, status as an actual or perceived victim of domestic violence, sexual assault, or stalking, or any other protected class as established by law.

Remitly is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.","Airflow
Add,Amazon Redshift
Add,Apache Spark
Add,Data Analytics
Add,Data Engineering
Add,Data Modeling
Add,Data Models
Add,Data Quality
Add,Extract, Transform, Load (ETL)
Add,Snowflake
Add,"
Senior Data Engineer,Bershaw · Philippines 2 weeks ago · 69 applicants,Remote Full-time Mid-Senior level,"About the job
Duties and Responsibilities:
Use your creativity to find and extract useful insights from Global Trade data Build and maintain scalable ETL pipelines that will perform complex transformations in a parallel fashion over large scale datasets 
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their needs. 
Spearhead the implementation of industry-standard world-class practices in data engineering 

Qualifications: 
• Experience in managing, analyzing and processing large volumes of sparsely structured data in a timely manner 
• Experience designing and building data warehouses and associated topologies 
• Strong experience with Relational, Non-Relational and Columnar Databases 
• Innovative “out of the box” thinking. This role is central to the company’s competitive advantage in providing unique insights using publicly available data 
• Ability to lead and manage a team 
• A positive “Can do” attitude 
• High attention to detail 
• Background in Statistical Analysis 
• Ability to liaise with management and other teams 
• Proficiency in working with cloud based data tools, specifically AWS 
• A basic grasp of statistics and probability will be a plus Requirements 
• Experience with SQL, RedShift, Spark, and Elasticsearch 
• Excellent written and verbal English communication skills 
• Driven with an intrinsic motivation to succeed and continuously improve yourself and your surroundings 
• Creative and able to find and build solutions to complex problems 
• Experience with Hadoop a plus","Amazon Redshift
Add,Analytical Skills
Add,Apache Spark
Add,Data Warehousing
Add,Databases
Add,Elasticsearch
Add,English
Add,Hadoop
Add,SQL
Add,Structured Data
Add,Teamwork
Add,"
Data Engineer,Kraken Digital Asset Exchange · Philippines 1 week ago · 71 applicants,Remote Full-time Entry level,"About the job
Building the Future of Crypto 

Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.

What makes us different?

Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you’ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken’s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.

Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.

As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.

Become a Krakenite and build the future of crypto!

Proof of work

The team

Join our Data Engineering Team at Kraken!

Are you passionate about leveraging the power of data to drive business decisions in the fast-paced world of cryptocurrency? We're seeking a talented Data Engineer to play a crucial role in our innovative Data Engineering team. As a Data Engineer at Kraken, you'll be at the forefront of transforming terabytes of data into actionable insights, enabling us to stay ahead in the ever-evolving cryptocurrency landscape.

If you are ready to be a driving force behind Kraken's data initiatives, working with cutting-edge technologies and a passionate team, we invite you to join us on this exciting journey at Kraken!

This role is fully remote.

The opportunity

Build scalable and reliable data pipeline that collects, transforms, loads and curates data from internal systems
Augment data platform with data pipelines from select external systems
Ensure high data quality for pipelines you build and make them auditable
Drive data systems to be as near real-time as possible
Support design and deployment of distributed data store that will be central source of truth across the organization
Build data connections to company's internal IT systems
Develop, customize, configure self service tools that help our data consumers to extract and analyze data from our massive internal data store
Evaluate new technologies and build prototypes for continuous improvements in data engineering.

Skills You Should HODL

4+ years of work experience in relevant field (Data Engineer, DWH Engineer, Software Engineer, etc)
Experience with data warehouse technologies and relevant data modeling best practices (Presto, Athena, Glue, etc)
Experience building data pipelines/ETL and familiarity with design principles (Apache Airflow is a big plus!)
Excellent SQL and data manipulation skills using common frameworks like Spark/PySpark, or similar.
Proficiency in a major programming language (e.g. Scala, Python, Golang,..) 
Experience with business requirements gathering for data sourcing.

Location Tagging: #EU #US #APAC

Kraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don't fully meet the listed requirements, especially if you're passionate or knowledgable about crypto!

As an equal opportunity employer, we don’t tolerate discrimination or harassment of any kind. Whether that’s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.

Stay in the know

Follow us on Twitter

Learn on the Kraken Blog

Connect on LinkedIn","Apache Spark
Add,Business Requirements
Add,DWH
Add,Data Engineering
Add,Data Manipulation
Add,Data Modeling
Add,Data Quality
Add,Data Science
Add,Extract, Transform, Load (ETL)
Add,PySpark
Add,"
Data Engineer,MONEYME · Metro Manila 2 weeks ago · 29 applicants,Hybrid Full-time Associate,"About the job
About MONEYME:

MONEYME is a founder-led digital lender and Certified B Corporation™. We challenge the traditional ways of credit and simplify the borrowing experience with digital-first experiences that meet the needs of modern consumers. We offer a range of fast, flexible, and competitively priced products that span our customers’ credit lifecycle, including personal loans, credit cards, and car loans. We deliver unrivalled customer experiences powered by smart technology, speed and efficiency.

We are for ambitious Australians that expect more from life and the companies they engage with. We uphold a strong ethos of sustainability and hold ourselves accountable to the high standards of the B Corp movement. Our culture is energetic and driven, and we continually challenge the status quo… we’re nothing like your traditional finance institution. We have recently been certified as a B Corp and a Great Place To Work. We’re proud to have built a culture where people feel heard, cared for, and empowered to push boundaries. We wouldn’t be able to continually improve and grow as a company without our diverse and exceptional team.

What we are looking for:

We are seeking a talented and motivated Data Engineer to join our dynamic team, reporting into the Business Intelligence Lead. As a key member of the Data Intelligence team, you will play a crucial role in designing, implementing, and maintaining our data infrastructure, with a focus on Azure Synapse and Power BI.

Responsibilities will include:

Data Pipeline Development
Azure Synapse / AWS ETL Management
Python and SQL Development
Performance Monitoring and Optimization
Documentation and Knowledge Sharing
Supporting the Business Intelligence Analysts
Support the Data Intelligence Team in key business projects by performing reconciliation and validation of data
Present ideas and relevant research to help us enhance current solutions and processes.
Assist the team in connecting Power BI to other useful data sources to provide users with self-serve access to data.
Assist the team (using relevant programming skills) to extract unstructured/semi-structured data from different file formats, such as JSON format stored in Cosmos DB.

To be successful for this role the following skills and experience is required:

Bachelor’s degree in computer science, Information Technology, or a related field.
Proven experience as a Data Engineer, with a focus on Azure Synapse, AWS Data Pipeline Service and Power BI.
Strong proficiency in Python and SQL for data processing and analysis.
Experience designing and optimizing data warehouses and ETL processes.
Familiarity with data modelling and schema design.

Other core skills:

Microsoft Azure certifications related to data engineering and Power BI.
Experience with other Azure services such as Azure Data Factory, Microsoft Fabric, etc.
Experience with other AWS services such as lambda, DynamoDB, S3 etc.
Familiarity with version control systems (e.g., Git).
Knowledge of best practices in data governance and security.
Strong team collaboration and communication skills
Excellent problem-solving skills, combined with accuracy and attention to detail.
The ability to work under pressure and adapt to change.
Excellent time management skills

What’s in it for you:

MONEYME’s employees and culture are core to who we are. We know that without a high-performing and engaged Team MONEYME, we will not achieve our ambitious goals for the future. We are proud to offer a collaborative and fun work environment.

Some of the benefits & perks we offer for all our employees in Manila are:

HMO on Day 1 + 1 free dependent
15 days of vacation leaves and 15 days of sick leave
1 birthday leave
Health and wellbeing initiatives like weekly sports activities and MONEYME Olympics
Fun filled company activities - summer outings, team building, team lunch or dinner, Halloween event, year-end party and so much more!
Complimentary snacks in the office
MONEYME Merchandise - hoodie, T-shirt, tumbler, notebook, and id lace
Quarter champion awards & reward trips

At MONEYME we believe in rewarding hard work. When the business is winning, so are you and we’re always investing in our employees to lead new projects and develop people’s careers. We have quarterly awards, events, bonuses and more.

MONEYME Limited is an equal opportunity employer and we value diversity, equity, and inclusion. We are committed to creating a diverse and inclusive workplace and encourage applicants from all backgrounds to apply. We believe that the unique contribution of our employees is a key driver of our success. We stand together – our diversity and inclusion give us an edge.","Amazon Web Services (AWS)
Add,Azure Data Factory
Add,Data Modeling
Add,Data Pipelines
Add,Data Warehousing
Add,Extract, Transform, Load (ETL)
Add,Git
Add,Microsoft Azure
Add,Python (Programming Language)
Add,SQL
Add,"
Data Engineer,"Security Bank Corporation · Makati, National Capital Region, Philippines Reposted 1 week ago · 553 applicants",Hybrid Full-time Entry level,"About the job
About Security Bank

We're one of the Philippines’ leading universal banks. Over the years, we received various awards and accolades for being one of the most stable in the banking industry.

As a Data Engineer, you will be responsible for developing strategies, managing execution timelines, and ensuring that project designs and implementations align with the data architectural vision and data governance standards as defined by Enterprise Data Architect and Data Governance Head.

What We’re Looking For

Bachelor's degree in Computer/Telecommunication, Industrial, Computer Science/Information Technology, Mathematics/Statistics, Economics, Finance/Accountancy/Banking
Knowledge of Data Warehouse and Data Marts concepts, Query optimization techniques, Python experience, and Data Analysis
Knowledge of Microsoft BI Suite (i.e, SSIS, SSRS, SSAS), PySpark, Python, Apache SQL, and Power BI.
Familiarity with AWS platform services such as S3, EC2, Athena, EMR

Fresh Graduates are welcome to apply!","Data Analytics
Add,Data Engineering
Add,Data Marts
Add,Data Science
Add,Data Warehousing
Add,Extract, Transform, Load (ETL)
Add,Optimization
Add,Optimization Techniques
Add,Query Optimization
Add,Query Writing
Add,"
Data Engineers,"NCS Group · Makati, National Capital Region, Philippines 3 weeks ago · 43 applicants",On-site Full-time Associate,"About the job
The Data Engineering Center of Excellence has been established in the DPM team to help realize the vision of becoming a customer-centric organization, driven by a data and analytics capability that enhances customer interactions and revenue generation.

The Big Data Engineer is responsible for development and automation of Data Lake ingestion, transformation and consumption services; adopting new technology; and ensuring modern operations in order to deliver consumer driven Data Lake solutions in both on-premises and Cloud platform implementations.

The role
Implement request for ingestion, creation, and preparation of data sources
Develop and execute jobs to import data periodically/ (near) real-time from an external source
Setup a streaming data source to ingest data into the platform
Delivers data sourcing approach and data sets for analysis, with activities including data staging, ETL, data quality, and archiving
Design a solution architecture on both On-premises and Cloud platforms to meet business, technical and user requirements
Profile source data and validate fit-for-purpose
Works with Delivery lead and Solution Architect to agree pragmatic means of data provision to support use cases
Understands and documents end user usage models and requirements

Preferred skills and experience include:
Bachelor’s degree in maths, statistics computer science, information management, finance or economics
At least 2 years’ experience integrating data into analytical platforms using patterns like API, files, XML, json, flatfiles, Hadoop file formats, and Cloud file formats.
Experience in ingestion technologies (e.g. sqoop, nifi, flume), processing technologies (Spark/Scala) and storage (e.g. HDFS, HBase, Hive) are essential
Experience in designing and building data pipelines using Cloud platform solutions and native tools.
Experience in Python, JVM-compatible languages, use of CICD tools like Jenkins, Bitbucket, Nexus, Sonarqube
Experience in data profiling, source-target mappings, ETL development, SQL optimisation, testing and implementation.
Expertise in streaming frameworks (Kafka/Spark Streaming/Storm) essential
Experience managing structured and unstructured data types
Experience in requirements engineering, solution architecture, design, and development / deployment
Experience in creating big data or analytics IT solution
Track record of implementing databases and data access middleware and high-volume batch and (near) real-time processing

Candidates must also be willing to work full onsite in Makati City.","Analytical Skills
Add,Azure Databricks
Add,Data Architecture
Add,Data Warehousing
Add,Engineering
Add,Microsoft Azure
Add,Programming
Add,Python (Programming Language)
Add,SQL
Add,Scala
Add,"
Data Engineer,"Avid · Manila, National Capital Region, Philippines Reposted 1 week ago · 176 applicants",Hybrid Full-time Entry level,"About the job
It's fun to work in a company where people truly BELIEVE in what they're doing!

We're committed to bringing passion and customer focus to the business.

Job Description

ABOUT AVID

Avid makes technology and collaborative tools so creators can entertain, inform, educate and enlighten the world. Our customers are the visionaries behind the most inspiring feature films, television programs, news broadcasts, televised sporting events, music recording and live concerts. To learn how Avid powers greater creators or for more information, visit www.avid.com.

Job Summary

The Data Engineer role will focus primarily on designing and implementing the data pipeline and common data models that will create the new data foundation for our enterprise analytics function. This will include building the ETL/ELT processes and orchestration layer that will drive the development of our Enterprise Data Warehouse and deliver data for analytics and systems integrations.

Responsibilities And Duties

Design, develop, implement, and support overall data eco-system which consolidates into an Enterprise Data Warehouse (EDW)
Maintain constantly evolving Data Vault data model which will serve the business data needs
Build and support ETL/ELT and data architectures within the data eco-system
Contribute to the constantly evolving Data Vault data model which will serve the business data needs
Execute and maintain standards and practices for adding new data sources into ecosystem, development, data quality, and support within the EDW
Ensure proper documentation, versioning, and data cataloging is performed for all data consumed & modelled within the EDW
Implementing comprehensive testing and monitoring processes to ensure data quality and timely error detection

Qualifications & Skills

Bachelor’s degree in computer science or related field
A minimum of 3-5 years EDW design/development experience or similar type role
Extensive experience with data related programming languages (SQL, python, redshift, etc) a MUST. 
Experience developing and maintaining a data warehouse environment within Snowflake.
Understanding of data warehouse architecture techniques (Data Vault 2.0 preferred) 
Strong experience with ELT/ETL development and tools (DBT experience preferred)
Familiarity with Bulk Data Integrations (FiveTran experience a plus) 
Understanding of data modeling practices and methodologies to deliver data for business use.
Strong analytical, troubleshooting, problem-solving and follow-through abilities, ability to navigate through ambiguous requirements
Have demonstrated verbal and written communication skills, and ability to interface with Business, Analytics, and IT organizations
Ability to work within a team and individually in a fast-paced agile environment

WHAT WE OFFER:

Here are some of the things you can count on when working for Avid:

Being part of one of the most recognized brands in the movie, music and broadcast business (when we perform, Earth shakes and superheroes fly!). 
A competitive salary, bonuses and benefits (such as private medical insurance, life insurance and sport package subsidies, depending on location). 
Full employment contract. 
Flexible working hours. 
A modern work from home policy. 
Visa sponsorship and relocation support (depending on the position). 

Avid is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.

If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!","Amazon Redshift
Add,Communication
Add,Computer Science
Add,Data Engineering
Add,Data Modeling
Add,Data Quality
Add,Extract, Transform, Load (ETL)
Add,Problem Solving
Add,Troubleshooting
Add,Written Communication
Add,"
Data Engineer,"JTI (Japan Tobacco International) · Taguig, National Capital Region, Philippines Reposted 1 week ago · 362 applicants",On-site Full-time Associate,"About the job
We’re JTI, Japan Tobacco International, and we believe in freedom.

We think that the possibilities are limitless when you’re free to choose. We’ve spent the last 20 years innovating and creating new and better products for our consumers to choose from. It’s how we’ve grown to be present in 130 countries, and how we’ve grown from 40 to 4,000+ employees in the Philippines since 2009.

But our business isn’t just business, our business is our people. Their talent. Their potential. We believe that when they’re free to be themselves, to grow, travel and develop, amazing things can happen for our business. That’s why our employees, from around the world, choose to be a part of JTI. It’s why 9 out of 10 would recommend us to a friend, and why we’ve been recognized as INVESTORS IN PEOPLE in the Philippines

It’s the perfect moment for you to #JoinTheIdea. We’re opening our Global Business Service center in the heart of BGC Manila and looking for more than 300 bright minds to join a global multinational with an exciting start-up vibe.

Title Data Engineer

Job ID  83284

Country  Philippines

City  Taguig City

Professional area Information Technology

Contract type  Permanent

Professional level  Experienced

Location Taguig City, 00, PH, 1630

DATA ENGINEER ASSOCIATE

Role works under the guidance of senior data engineers to maintain and develop pipelines in the JTI’s Central Data Lake Platform. The incumbent will analyze, organize, and combine raw data from different sources and build the corresponding data systems and pipelines to ensure that business needs and objectives are met with the data delivered.

Develop and maintenance of data pipelines, which includes 

 Develop the data pipelines for the corresponding projects ensuring that are highly available, scalable, reliable, secure, and cost-effective
 Support technical design, development, unit testing, and production deployment.
 Design and document the pipelines developments
 Writes unit/integration tests, contributes to engineering wiki, and documents work.
 Supports user testing during UAT phase and work on bug fixing
 Support to Roll outs and deployments.
 Ensure Data pipelines maintenance and performance testing.

What will you do?

Platform Configuration & Developments
 Ensure that Data Platform standards are applied and adopt new ones coming as part of expected platform evolution
 Support technical design, development and unit testing, and production deployment. Ensure that the solution is aligned with the agreed business process design, functional design and with Microsoft Azure best practices and JTI architecture standards
 Promote DEV2QA2PRD changes, support user testing, fine tune the solution based on testing feedback
 Ensure consistency & standardization of the data pipelines
 Ensure that required system documentation is completed and delivered prior to Go-Live. Contribute to Knowledge Base creation.
Issue resolution and operational support
 Provide timely technical support for issue resolution including UAT and Post Go-Live support, including data maintenance and user access administration. Work with end users and other support teams and the vendor if required
 Closely collaborate with Data & Analytics, BTS, Enterprise Architecture or external vendors.
 Timely update assigned tasks, provide response and solution within agreed team's timelines
 Daily monitor for running pipelines ensuring correctness of the execution and no business disruption
 Raise and investigate Incidents in IT Service Portal

Who are we looking for?

 1+ year of experience in Data and Analytics from modelling and reporting standpoint
 1+ exposure to IT architecture and solutioning for data flows among system and applications.
 1+ year delivering solutions on the Microsoft Azure platform with a special emphasis on data solutions and services
 Fast learner and able to embrace new technologies
 Experience with ETL tools preferable Azure Data Factory and DataBricks (or similar)
 Azure Data Lakes experience is a must
 Strong analytical skills and troubleshooting skills
 Should have good knowledge in testing and validating data solutions
 Database skills, scripting, and data modeling
 Very good written and spoken English.
 Good analytical and communication skills.
 Good knowledge of software development lifecycle and best practices.
 Knowledge of defining and rolling out standards and templates.
 The ability to work effectively in a diversified team under pressure situations.
 Offshore and nearshore management experience

What's in it for you?

Work in a brand-new office located in BGC McKinley West
Be covered with medical insurance upon hiring, with dependent coverage and medicine allowance
Receive cash allowances such as meal and transport allowance
Flexible working arrangements
Have access to over 200 company training
Be part of a truly international and diverse company with over 40,000 employees in 130 countries.
Experience the culture of an Investors in People certified company
Find out why 9 out of 10 employees recommend us to a friend.
Understand why 9 out of 10 employees say they feel free to be themselves

What Are The Next Steps - Recruitment Process

Thank you very much for applying!

We will make sure to provide feedback on your application within 2 weeks after the application deadline.","Communication
Add,Data Analytics
Add,Data Engineering
Add,Data Maintenance
Add,Data Modeling
Add,Databases
Add,Extract, Transform, Load (ETL)
Add,IT Documentation
Add,Technical Design
Add,Troubleshooting
Add,"
Data Engineer,"QBE Insurance · Manila, National Capital Region, Philippines 5 months ago · 67 applicants",Hybrid Full-time Entry level,"About the job
Primary Details

Time Type: Full time



Worker Type: Employee

Primary Responsibilities


 Create and maintain optimal data pipeline architecture
Assemble complex data sets that meet business requirements.
Data cleaning and normalization
Identifying proper data features that will increase models accuracy
Optimise extraction, transformation, and loading of data
Create data tools for analytics and machine learning team members that assist them in optimizing their model



Required Education


 Bachelor's Degree or equivalent combination of education and work experience



Required Experience


 2 years relevant experience



Preferred Competencies/Skills


 Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing data pipelines, architectures and datasets
Strong analytic skills related to working with unstructured datasets
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Conceptual knowledge/background in Agile (Scrum) practice



Preferred Experience


 At least 2 years of experience in modeling system, and statistical and probabilistic analysis



Preferred Knowledge


 BSc in Computer Science, Mathematics, Statistics or similar field; Master’s degree is a plus



How to Apply:

To submit your application, click ""Apply"" and follow the step by step process.

Equal Employment Opportunity:

QBE is an equal opportunity employer and is required to comply with equal employment opportunity legislation in each jurisdiction it operates.","Analytical Skills
Add,Analytics
Add,Business Requirements
Add,Data Cleaning
Add,Data Engineering
Add,Data Warehousing
Add,Databases
Add,Datasets
Add,Extract, Transform, Load (ETL)
Add,Query Writing
Add,"
Senior Data Engineer,Ceridian · Philippines 2 weeks ago · 22 applicants,Remote Full-time Mid-Senior level,"About the job
Location: Work is what you do, not where you go. For this role, we are open to remote work and can hire anywhere in the United States or Canada

About The Opportunity

The Enterprise Data Team is looking for a Data Solutions Developer Sr responsible for building complex enterprise data that the business requires to drive actionable insights; transforming, modelling, processing and extracting value from datasets; utilizing cutting-edge technology to provision the data to the business to enable business self service

What You’ll Get To Do

Integrate, transform, and consolidate data from various structured and unstructured data systems into structures that are suitable for building analytics solutions. 
Ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints
Bring data into the Data Lake
Bring data from raw to cleansed to modelled
Design and maintain serverless and dedicated SQL pools 
Design and maintain dataflows
Troubleshoot and resolve advanced issues with existing pipelines, models and dataflows
Assemble large and complex data models using data warehouse best practices
Document data processes and models
Employ a variety of languages and tools to query databases, create scripts, and marry data across multiple systems together
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Develop processes and tools to monitor and analyze model performance and data accuracy
Create Proof of Concepts
Collaborate with cross-functional teams to deliver analytics solutions
Empower users to bring data to life through self-service models and Business Intelligence Analytics tools
Participate in architecture review
Participate in data governance practices
Participate in MDM practices
Provide mentorship to the Data Solutions Developer

What’s In It For You

Encouragement to be the best version of yourself at and away from work:

YOUnity diversity and inclusion programs
Amazing time away from work programs 
Support for your total well-being through our Live Well, Work Well programs targeting all aspects of your life
Recognition for your contributions through excellent pay, perks, and rewards
Giving where you’re living: volunteer days, Ceridian sponsored events, and our very own charity, Ceridian Cares
Opportunities to fuel your career growth through numerous internal and external programs and events

Skills And Experience We Value

A Bachelor’s Degree in Computer Science, Engineering, Applied Math or Business or equivalent degree and experience
5-10 years of experience working with data
Solid knowledge of data languages such as T-SQL, Python or Pyspark, Spark SQL and DAX
Strong knowledge of Azure Data Factory, Synapse Pipelines, SSIS or other ETL/ELT tools
Experience with data modelling, data warehousing, and business intelligence architecture
Experience with cloud storage solutions and big data file formats
Experience with Delta Lake infrastructure and manipulation techniques via Databricks or Synapse
Knowledge of Serverless Pool and Dedicated Pool concepts
Experience using DevOps GIT for source control and CI/CD
Experience with business intelligence tools such as Power BI and Tableau
Proven track record of quickly learning new technologies

What Would Make You Really Stand Out

Experience with many cloud data platforms such as AWS, Azure, Snowflake
Experience with big data solutions","Big Data
Add,Computer Science
Add,Data Analytics
Add,Data Modeling
Add,Data Models
Add,Data Pipelines
Add,Data Warehousing
Add,Extract, Transform, Load (ETL)
Add,Snowflake
Add,Snowflake Cloud
Add,"
"Principal Consultant, AWS Cloud Engineer","Genpact · Makati, National Capital Region, Philippines 2 weeks ago · 14 applicants",Hybrid Full-time Mid-Senior level,"About the job
With a startup spirit and 115,000 + curious and courageous minds, we have the expertise to go deep with the world’s biggest brands—and we have fun doing it! We dream in digital, dare in reality, and reinvent the ways companies work to make an impact far bigger than just our bottom line. We’re harnessing the power of technology and humanity to create meaningful transformation that moves us forward in our pursuit of a world that works better for people.
Now, we’re calling upon the thinkers and doers, those with a natural curiosity and a hunger to keep learning and keep growing. People who thrive on fearlessly experimenting, seizing opportunities, and pushing boundaries to turn our vision into reality. And as you help us create a better world, we will help you build your intellectual firepower.

We are inviting applications for the role of Principal Consultant, AWS Cloud Engineer!

We are looking for an energetic and enthusiastic professional with a solid technical foundation in AWS and a proven track record for delivering high-quality services in the Cloud. As a key member of a distributed team, you will build and maintain distributed systems that provide Identity and API gateway services, using industry best practices. This includes building and maintaining high-quality monitoring and alerting tools and automating delivery processes. You'll be responsible for architecture, design, development, testing, and operations and deliver incrementally in iterative cycles. You will have an Agile mindset and a keen interest in software engineering across a range of technologies. We also want your commitment to providing on-call support (on a rotation basis) and to continuous uptime for client-facing services.
In addition, you will also be expected to be a technological thought leader within the team to identify and drive the uptake of new technologies where appropriate.

Responsibilities 

Take a lead role in architecting and designing distributed systems, specifically focusing on Identity and API gateway services, adhering to industry best practices.
Engage in the end-to-end development lifecycle, from coding to testing, ensuring the delivery of high-quality services. Implement solutions iteratively and incrementally.
Oversee the operational aspects of the systems, ensuring continuous uptime for client-facing services. Implement and maintain robust monitoring and alerting tools to promptly identify and address any issues.
Develop and maintain automated delivery processes, streamlining deployment and enhancing efficiency. Implement automation tools and practices to facilitate smooth operations.
Embrace an Agile mindset and actively contribute to the distributed team. Collaborate effectively with team members to deliver projects incrementally in iterative cycles.
Demonstrate commitment by providing on-call support on a rotation basis. Respond promptly to incidents and contribute to minimizing downtime for client-facing services.
Act as a technological thought leader within the team. Identify and advocate for the adoption of new technologies where applicable, contributing to the team's innovation and growth.
Stay informed about advancements in technology and industry best practices. Proactively seek opportunities for continuous learning and improvement, applying new knowledge to enhance system performance and capabilities.

Qualifications we seek in you! 

Minimum Qualifications / Skills
competency in modern scripting and automation (UNIX/shell scripting or Python)
experience Amazon Web Services (AWS), including proficiency in cloud formation config
experience in Web and API technologies (including REST, HTTP & cert management, API gateway proxies)
experience with Continuous Integration and Delivery pipeline (Ex: Git, Stash, Bamboo, etc.)
experience with Configuration management tools (ex: Ansible) and containerization technologies (ex: Docker)

Preferred Qualifications/ Skills
Security broker (IBM Security Access Manager, LDAP)
API Gateway technologies such as Apigee
Application Development (Java) and microservices
Monitoring and alerting tools (ex: Sumologic, AppDynamics, Splunk)

Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com. Follow us on Twitter, Facebook, LinkedIn, and YouTube.
Also, please keep in mind that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.","Amazon Web Services (AWS)
Add,Ansible
Add,Docker Products
Add,Git
Add,Java
Add,LDAP
Add,Microservices
Add,Python (Programming Language)
Add,Shell Scripting
Add,Splunk
Add,"
Data Engineer (Morning Shift),"Eviden · Taguig, National Capital Region, Philippines 1 week ago · 33 applicants",Hybrid Full-time Mid-Senior level,"About the job
Set-Up

Night Shift
Oniste
Office is loated in Taguig CIty

Job Description
Technical proficiency
Big Data & Data warehouse technologies
 Apache Nifi
Apache Hive
Apache HBase
Apache Spark
· Programming languages
Scala
SQL
· Other nice to have technical skills:
Ø Experience using JIRA in an Agile (Scrum) project to document Epics, Features and User Stories
Ø Experience using Bitbucket or similar GIT Source Version Control system is appreciable
Ø Experience using SonarCube is a nice to have","Apache NiFi
Add,Apache Spark
Add,Data Warehousing
Add,HBase
Add,Hadoop
Add,Hive
Add,Jira
Add,MapReduce
Add,NoSQL
Add,SQL
Add,Scala
Add,"
"Data Engineer- Python, SQL, MS Azure","QBE Group Shared Services Centre · Manila, National Capital Region, Philippines 2 weeks ago · 21 applicants",Hybrid Full-time Associate,"About the job
Primary Details

Time Type: Full time
Worker Type: Employee

Responsible as a team member for assembling and gathering complex data sets that meet business requirements for Machine Learning and Data Science. Optimising ETD(Extraction, Transformation, Loading) and cleaning of Data.

Primary Responsibilities 

• Create and maintain optimal data pipeline architecture
•Assemble complex data sets that meet business requirements.
•Data cleaning and normalization
•Identifying proper data features that will increase models accuracy
•Optimise extraction, transformation, and loading of data
•Create data tools for analytics and machine learning team members that assist them in optimizing their model

Required Education 

• Bachelor's Degree or equivalent combination of education and work experience

Required Experience 

• 2 years relevant experience

Preferred Competencies/Skills 

• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
•Experience building and optimizing data pipelines, architectures and datasets
•Strong analytic skills related to working with unstructured datasets
•A successful history of manipulating, processing and extracting value from large disconnected datasets.
•Conceptual knowledge/background in Agile (Scrum) practice

Preferred Experience 

• At least 2 years of experience in modeling system, and statistical and probabilistic analysis

Preferred Knowledge 

• BSc in Computer Science, Mathematics, Statistics or similar field; Master’s degree is a plus


QBE Cultural DNA 

• Everything we do at QBE is underpinned by our DNA (which interlinks seven cultural elements) – because we know it's not just what we do that matters, it's how we do it that makes the difference. We expect all employees to role model and inspire the right behaviours that link to our cultural elements:
•We are customer-focused
•We are technical experts
•We are inclusive
•We are fast-paced
•We are courageous
•We are accountable
•We are a team
•All employees are expected to adhere to QBE’s Code of Ethics and Conduct and apply sound risk management practices

US Only - Disclaimer 

• To successfully perform this job, the individual must be able to perform each essential job responsibility satisfactorily. Reasonable accommodations may be made to enable an individual with disabilities to perform the essential job responsibilities.

Job Type 

• Individual Contributor

Global Disclaimer 

• The duties listed in this job description do not limit the assignment of work. They are not to be construed as a complete list of the duties normally to be performed in the position or those occasionally assigned outside an employee’s normal duties. Our Group Code of Ethics and Conduct addresses the responsibilities we all have at QBE to our company, to each other and to our customers, suppliers, communities and governments. It provides clear guidance to help us to make good judgement calls.
How to Apply:
To submit your application, click ""Apply"" and follow the step by step process.
Equal Employment Opportunity:
QBE is an equal opportunity employer and is required to comply with equal employment opportunity legislation in each jurisdiction it operates.","Analytical Skills
Add,Business Requirements
Add,Computer Science
Add,Customer-Focused Service
Add,Mathematics
Add,Microsoft Azure
Add,Python (Programming Language)
Add,Relational Databases
Add,SQL
Add,Statistics
Add,"
Data Engineer,"Manulife Philippines · Quezon City, National Capital Region, Philippines Reposted 1 day ago · 33 applicants",Hybrid Full-time Mid-Senior level,"About the job
Are you looking for a supportive and collaborative workplace with great benefits and clear career development? You’ve come to the right place. 

Why choose Manulife? 
Competitive Salary packages and performance bonuses 
Day 1 HMO + FREE coverage for your dependents (inclusive of same-sex partners) 
Retirement savings benefit 
Rewarding culture that values wellness and well-being 
Performance Bonus 
Global network of industry experts 
Extensive training resources 
 Job Description: 
We’re looking for a Data Engineer to design and develop data pipelines and ETL jobs using Big Data Technologies. Get the chance to work with our team of experts and with key business stakeholder to transform business data into projects with real-world impact. 

Have the skills and knowledge for the job? Learn more about the opening below! 

Key Responsibilities: 
Design and develop data pipelines and ETL jobs using Big Data Technologies based on functional/non-functional business requirements 
Design and implement Data Integration/Ingestion/Extraction solutions based on high level architecture design 
Identify, design, and implement process improvements and delivery optimizations 
Collaborate with Stakeholders, Business Analysts, and Data Architects to assist and translate business requirements to technical solutions 
Develop big data and analytic solutions leveraging new or existing technology to advance Manulife’s all lines of business 
Exploratory data analysis; Query and process on-premise or cloud-based data, provide reports, summarize, and visualize the data 
Design, upgrade, and implement new data workflows, automation, tools, and API integrations 
Perform POC on new integration patterns and solutions 
Write and maintain technical documentation 
Perform unit tests and system integration tests 
Executes updates, patches, and other activities required to maintain and enhance the operations of on-premise or cloud-based environments 
 Qualifications: 
At least 2 years experience as Data Engineer, with focus on Big Data processing and/or relational databases 
At least 2 years experience working with Microsoft Azure Data Platform, specifically Azure SQL Database, Azure Data Factory, ADLS storage account 
Experienced with working on Structured, Semi-Structured, and Unstructured datasets 
Experienced with any of the Big Data tools and technologies: 
Hadoop, Spark, Hive, Sqoop, Kafka, Nifi 
Experienced with relational SQL and NoSQL databases: 
MSSQL, Postgres, HBase, (MongoDB is a plus)big data 
Experienced in creating data pipelines and in developing complex and optimized queries 
Experienced with Workflow Management Tools: Airflow, Crontab, CA Workload Automation 
Knowledgeable with CI/CD tools 
Experienced in any of the following programming/scripting languages: 
SQL, Python, Shell, Scala 
Basic knowledge of Data Visualization in any of the following tools: 
Tableau, PowerBI, QlikView/QlikSense 
Knowledgeable in using collaboration tools (eg., MS Teams, Skype, Confluence, JIRA, etc.) 
Experience with any SLDC Methodologies and familiarity with different Agile methodologies 

Let's make every day better together. Learn about our opportunities at JOBS.MANULIFE.COM","Apache NiFi
Add,Azure Data Factory
Add,HBase
Add,Hive
Add,Microsoft Azure
Add,MongoDB
Add,Python (Programming Language)
Add,SQL
Add,Scala
Add,Sqoop
Add,"
Data Engineer - QuickSight (Homebased 2106465966),Outsourced · Philippines 4 weeks ago · 39 applicants,Remote Full-time Mid-Senior level,"About the job
The role will focus on collaborating with the business to develop data models, process flows, datasets and views that are efficient and meaningful representations of data, optimising the data integration platform and ensuring performance, data availability, accessibility, and integrity. You will also be heavily involved in data integration, data consolidation mechanisms and other artefacts necessary to implement best practice ETL type integrations into a data warehouse and data lakes. The role will also develop SQL stored procedures and functions for various in-house applications.

As the Data Engineer, you will provide technical expertise across data platforms to support initiative delivery and new technology implementations. You will oversee the management, optimising, and monitoring of data retrieval, storage and distribution. You will play a key role in supporting the data infrastructure & warehousing.

Main Duties/ Responsibilities

Data Engineer responsibilities include:

Assist in providing feedback as to how data should be captured in systems to ensure the best reporting outcomes are possible
Responsible for advanced SQL queries to display provide access to data as needed
Combine raw information from different sources and data preparation in SQL or tools like QuickSight
Design and implement efficient data flow solutions between different systems
Create a data catalogue to help organise and expose data to analysts
Collaborate with Data Analysts on data requirements
Producing advanced data visualisation using data storytelling skills

What is you experience?

The candidate for this position must also have had at least five years of working experience in a data-focused engineering role; they will also have had experience extracting data and creating queries, reports, and dashboards for decision-makers.

Skills & Experiences

Tertiary qualifications in Computer or Data Science or equivalent
Proficient in SQL (developing complex SQL outputs taking performance into account)
Data Warehousing experience, including multi-dimensional and tabular models that integrate with a range of systems
Knowledge of AWS environment (including QuickSight)
Strong ETL experience
Good understanding of agile methodologies.
Excellent communication (written and oral) and team-work skills.
Domain expertise in education or educational software is desirable.

 Key data technologies currently: 

QuickSight reporting and dashboards
PostgreSQL (AWS RDS)
SFDC reporting and dashboards

Work Schedule 

Monday-Friday 7am-4pm Manila Time

Location 

Homebased; must have minimum of 20 mbps internet speed for both upload/download.

By clicking on the ""I'm Interested"" button I hereby allow Outsourced Quality Assured Services, Inc. (""Outsourced"") to store and collect my personal information for the purposes of employment application. As such, I agree and authorize Outsourced to collect, store, or continue to use my personal information for the above-stated purpose, and to retain my personal information for a period of 1 year, and for these purposes only.","Amazon Relational Database Service (RDS)
Add,Data Infrastructure
Add,Data Preparation
Add,Data Science
Add,Data Storytelling
Add,Data Visualization
Add,Data Warehousing
Add,Extract, Transform, Load (ETL)
Add,Optimising
Add,SQL
Add,"
Data Engineer,"BDO Unibank · National Capital Region, Philippines 2 weeks ago · 57 applicants",On-site Full-time Mid-Senior level,"About the job
He/She/They will be responsible in ensuring that the data is in place to support the growth of digital channels (product) and digital marketing, which enable them to meet the organization’s business objectives.

The Responsibilities we will trust you with:

Manage and maintain the digital team’s sandbox in accordance to the needs of the business
Design and develop effective solutions for the data team’s data and information needs
Support the data requirements of BDO’s digital products

Your Qualifications and Your Experiences should be:
  Bachelor’s degree in Computer Science, Engineering, Mathematics or Statistics or relevant courses
At least 5 years of experience in data technology experience (data architecture, data engineering, data platforms, data mining, data warehouse, etc.)
Experience with integration methods and tools including ETL and virtualization, ETL processing using Azure Databricks and Delta Lake including optimization
Experience working with structured and unstructured data
Highly proficient with the use of the following programming languages SQL, Python
Knowledge in BI tools (PowerBI, Tableau)

You must be willing to work on site and be assigned in Ortigas.","Azure Databricks
Add,Computer Science
Add,Data Architecture
Add,Data Engineering
Add,Data Mining
Add,Data Modeling
Add,Data Warehousing
Add,Extract, Transform, Load (ETL)
Add,Mathematics
Add,Microsoft Azure
Add,"
Senior Data Engineer,"SGV & Co. · Makati, National Capital Region, Philippines 5 days ago · 14 applicants",Hybrid Full-time Mid-Senior level,"About the job
The opportunity

In the Senior Data Architecture and Engineering role, you will be part of a culture that focuses on applying a set of solution that uses methods, processes, and technologies to design, build, and operate scalable on-premises or cloud data architecture and modelling solutions that facilitate data storage, integration, management, validation, and security, supporting the entire data asset lifecycle.
We work with our clients to understand their data environment and help them through their journey to become a data-driven organization.


Your key responsibilities

Design, build, and operate data integration solutions that optimize data flows by consolidating disparate data from multiple sources into a single solution
Work with other Information Management and Analysis professionals, the program team, management and stakeholders, to design and build analytics solutions in a way that will deliver business value.
Contribute to the delivery of one or more processes, solutions and/or projects by delivering agreed activities, applying judgement and selecting appropriate methodologies to inform recommendations, considerate of success criteria, barriers, risks and issues
Be accountable for meeting own targets which impact the immediate team
Coordinate the work of junior colleagues or team members
Meets performance objectives and metrics set locally (client service, quality and risk management, sales and business growth, solution development and teaming etc.)


Skills and capabilities for success
 Cloud computing: understands cloud technologies such as virtualized environments, Big Data storage, and AI services; ability to support custom application development efforts
Data integration: ability to provide data integration solutions that collect and combine data from different sources into consolidated views, while maintaining data quality, accuracy, cleanliness and integrity so as to make the data available for trusted consumption by analytics processes and defined stakeholders
Data fabric architecture: knowledge of and ability to apply enterprise data fabric approaches, focused on creating business value by providing a single, integrated and consistent view of data, decoupled from storage
Data security: knowledge of and ability to provide data security solutions that enable data monitoring and protection from unauthorized access and corruption throughout the data lifecycle
Enterprise data management: knowledge of and ability to design and/or apply a comprehensive collection of methods, practices, policies and processes to manage the entire lifecycle of all data assets of a company, ensuring their protection, governance, trusted consumption, and monetization
Data architecture design and modeling: knowledge of and ability to develop a robust and coherent data strategy, support architectures, modeling solutions, policies, practices and procedures that are scalable, efficient and enable clients to manage and utilize data in an effective manner
Computational thinking and programing: knowledge of and ability to use computational models, tools and techniques to interpret and understand data, solve problems and formulate solutions
Data quality: knowledge of and ability to provide quality assessments that leverage a disciplined data governance framework, guidance for data intake, data cleansing and data presentation
Database management: knowledge of and ability to provide database installation, configuration, upgrades, maintenance and disaster recovery strategies
Semantic layer: knowledge of and ability to understand, build, maintain and work with semantic models to provide business users access to complex data and computations

Business skills we expect you to develop:

Building and managing relationships
Communication and presentation skills
Critical and analytical thinking
Driving outcomes
Teaming and hybrid collaboration
Stakeholder management
Complex problem solving
Digital fluency
Emotional agility
Learning agility

Job requirements:
Demonstrate familiarity with basic cloud concepts and features, data structures, algorithms and visualization
Demonstrate an intermediate understanding of the principles, methods, processes and standards of integration, as well as tools used in the design and build of data integration solutions
Participate in designing blueprints on how to structure, store and utilize data
Plan processes for effective data storage, sharing and utilization within the organization
Demonstrate application of principles, methods, processes and standards to support the realization of atypical computational thinking and programming requirements, with supervision
Understand the foundation principles, methods, processes and standards of data fabric and demonstrates a working knowledge of tools used in design and build of:
o Data fabric
o Data security
o Enterprise data management
o Data management
o Semantic layer
Experience in execution of structured deliverables and able to solve simple problems on:
o Data fabric
o Data security
o Enterprise data management
o Data management
o Semantic layer

Experience in stakeholder management
Ability to monitor a large amount of time sensitive and complex client communications
Ability to analyze potential efficiency improvements for existing business processes
Possess a growth mindset to support continuous improvement and contribute to business development
Ability to build effective, long-lasting relationships with clients

What we look for
You have an agile, growth-oriented mindset. What you know matters. But the right mindset is just as important in determining success. We’re looking for people who are innovative, can work in an agile way and keep pace with a rapidly changing world, can quickly understand fast-moving situations, demonstrates self-awareness when dealing with others, and adapts own approach based on differences in individual styles and backgrounds.
You are curious and purpose driven. We’re looking for people who see opportunities instead of challenges, who ask better questions to seek better answers that build a better working world.
You are inspiring. We’re looking for people who communicate with confidence, integrity and authenticity to build strong, positive relationships based on trust and grow professionally through learning, coaching and new experiences.
You are inclusive. We’re looking for people who seek out and embrace diverse perspectives, who value differences, and team inclusively to build safety and trust.

What we offer
Continuous learning: You’ll develop the mindset and skills to navigate whatever comes next.
Success as defined by you: We’ll provide the tools and flexibility, so you can make a meaningful impact, your way.
Transformative leadership: We’ll give you the insights, coaching and confidence to be the leader the world needs.
Diverse and inclusive culture: You’ll be embraced for who you are and empowered to use your voice to help others find theirs.
If you can demonstrate that you meet the criteria above, please contact us as soon as possible.



 The exceptional EY experience. It’s yours to build. 
EY | Building a better working world

EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.
Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.

Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.","Analytical Skills
Add,Client Relations
Add,Cloud Computing
Add,Data Modeling
Add,Data Structures
Add,Databases
Add,Learning Agility
Add,Presentation Skills
Add,Problem Solving
Add,Software Solution Development
Add,Stakeholder Management
Add,"
